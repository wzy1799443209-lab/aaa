import streamlit as st
import google.generativeai as genai
import os
from dotenv import load_dotenv

# 1. åŠ è½½ç¯å¢ƒå˜é‡ (å¦‚æœä½ åœ¨æœ¬åœ°æœ‰ .env æ–‡ä»¶)
load_dotenv()

# 2. é¡µé¢åŸºç¡€é…ç½®
st.set_page_config(
    page_title="AI Soul Studio",
    page_icon="ğŸ’¬",
    layout="centered" # æ‰‹æœºç«¯ä½“éªŒæ›´ä½³
)

# --- ä¾§è¾¹æ é…ç½®åŒº (æ¨¡æ‹Ÿå¾®ä¿¡çš„è®¾ç½®) ---
with st.sidebar:
    st.header("âš™ï¸ çµé­‚å‚æ•°è®¾ç½®")
    
    # API Key è¾“å…¥ (ä¼˜å…ˆè¯»å–ç¯å¢ƒå˜é‡ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ‰‹åŠ¨è¾“å…¥)
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        api_key = st.text_input("è¯·è¾“å…¥ Gemini API Key", type="password")
    
    st.markdown("---")
    
    # è§’è‰²è®¾å®š
    target_name = st.text_input("AI çš„åå­—", value="å¥¹")
    user_name = st.text_input("ä½ çš„åå­—", value="æˆ‘")
    
    # æ ¸å¿ƒï¼šè¿™é‡Œæ˜¯æœªæ¥æ”¾å…¥æ¸…æ´—åæ•°æ®çš„åœ°æ–¹
    # ç›®å‰ä½œä¸º MVPï¼Œæˆ‘ä»¬å¯ä»¥æ‰‹åŠ¨å†™ä¸€æ®µ Prompt æ¥æµ‹è¯•
    default_prompt = f"""
    ä½ ç°åœ¨éœ€è¦è¿›è¡Œè§’è‰²æ‰®æ¼”ã€‚
    ä½ çš„åå­—æ˜¯{target_name}ï¼Œä½ æ­£åœ¨å¾®ä¿¡ä¸Šå’Œ{user_name}èŠå¤©ã€‚
    
    ã€æ€§æ ¼ç‰¹å¾ã€‘ï¼š
    æ¸©æŸ”ã€æœ‰æ—¶å€™æœ‰ç‚¹å°è°ƒçš®ï¼Œå–œæ¬¢ç”¨æ³¢æµªå·~ï¼Œä¸å–œæ¬¢å›å¤ªé•¿çš„å­—ã€‚
    
    ã€è¯´è¯æ ·æœ¬ã€‘ï¼š
    {user_name}: æ™šä¸Šåƒäº†å—ï¼Ÿ
    {target_name}: è¿˜æ²¡å‘¢ï¼Œåˆšä¸‹ç­ï¼Œé¥¿æ™•äº†éƒ½[æµæ³ª] ä½ å‘¢ï¼Ÿ
    {user_name}: æˆ‘ä¹Ÿæ²¡ã€‚
    {target_name}: é‚£æˆ‘ä»¬è¦ä¸è¦å»åƒé‚£ä¸ªç«é”…å‘€ï¼Ÿå¥½ä¹…æ²¡å»äº†ï¼
    
    è¯·ä¸¥æ ¼æ¨¡ä»¿ä¸Šè¿°è¯­æ°”ä¸æˆ‘å¯¹è¯ã€‚ä¸è¦åƒä¸ªæœºå™¨äººï¼Œè¦ç”Ÿæ´»åŒ–ã€‚
    """
    
    system_instruction = st.text_area(
        "çµé­‚æŒ‡ä»¤ (System Prompt)", 
        value=default_prompt, 
        height=300,
        help="åœ¨è¿™é‡Œç²˜è´´ä½ æ¸…æ´—å¥½çš„èŠå¤©è®°å½•æ ·æœ¬ï¼ŒAI ä¼šæ¨¡ä»¿è¿™ç§è¯­æ°”ã€‚"
    )
    
    temperature = st.slider("æƒ…æ„Ÿæ³¢åŠ¨ (Temperature)", 0.0, 1.0, 0.7, help="å€¼è¶Šé«˜ï¼ŒAI è¶Šæœ‰åˆ›é€ åŠ›ï¼›å€¼è¶Šä½ï¼ŒAI è¶Šä¸¥è°¨ã€‚")
    
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºèŠå¤©è®°å¿†"):
        st.session_state.messages = []
        st.rerun()

# --- 3. åˆå§‹åŒ–èŠå¤©å¼•æ“ ---

if api_key:
    try:
        genai.configure(api_key=api_key)
        # ä½¿ç”¨ gemini-1.5-flashï¼Œé€Ÿåº¦å¿«ä¸”å…è´¹é¢åº¦é«˜ï¼Œé€‚åˆèŠå¤©
        model = genai.GenerativeModel(
            model_name="gemini-2.5-flash",
            system_instruction=system_instruction
        )
    except Exception as e:
        st.error(f"API Key é…ç½®å‡ºé”™: {e}")
else:
    st.warning("è¯·åœ¨ä¾§è¾¹æ è¾“å…¥ Google Gemini API Key æ‰èƒ½å¼€å§‹èŠå¤©ã€‚")
    st.stop()

# --- 4. èŠå¤©ç•Œé¢é€»è¾‘ ---

st.title(f"ğŸ’¬ ä¸ {target_name} çš„èŠå¤©")

# åˆå§‹åŒ– Session State (ç”¨äºå­˜å‚¨ç½‘é¡µåˆ·æ–°åçš„èŠå¤©è®°å½•)
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    # åŒºåˆ† User å’Œ Assistant çš„å¤´åƒ
    avatar = "ğŸ§‘â€ğŸ’»" if message["role"] == "user" else "âœ¨"
    with st.chat_message(message["role"], avatar=avatar):
        st.markdown(message["content"])

# --- 5. å¤„ç†ç”¨æˆ·è¾“å…¥ ---

if prompt := st.chat_input("å‘æ¶ˆæ¯..."):
    # A. æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»"):
        st.markdown(prompt)

    # B. AI ç”Ÿæˆå›å¤
    try:
        # å°† Streamlit çš„å†å²è®°å½•è½¬æ¢ä¸º Gemini çš„æ ¼å¼
        history_for_gemini = [
            {"role": "user" if m["role"] == "user" else "model", "parts": [m["content"]]}
            for m in st.session_state.messages[:-1] # ä¸åŒ…å«æœ€æ–°çš„ä¸€æ¡ï¼Œå› ä¸ºæˆ‘ä»¬ä¸‹é¢è¦å•ç‹¬å‘
        ]
        
        chat = model.start_chat(history=history_for_gemini)
        
        with st.chat_message("assistant", avatar="âœ¨"):
            # ä½¿ç”¨æµå¼è¾“å‡º (Stream) æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ
            response_placeholder = st.empty()
            full_response = ""
            
            # å‘é€æ¶ˆæ¯
            response_stream = chat.send_message(prompt, stream=True, generation_config={"temperature": temperature})
            
            for chunk in response_stream:
                if chunk.text:
                    full_response += chunk.text
                    response_placeholder.markdown(full_response + "â–Œ")
            
            response_placeholder.markdown(full_response)
        
        # C. ä¿å­˜ AI å›å¤åˆ°å†å²
        st.session_state.messages.append({"role": "assistant", "content": full_response})

    except Exception as e:
        st.error(f"å‘ç”Ÿé”™è¯¯: {e}")